#!/usr/bin/env python3
"""
Comprehensive AI Insights API Test Suite

This test suite validates all the AI insights API fixes that have been implemented:
1. Fixed Critical Issues:
   - Added to_dict() method to DebtInDB model (should fix refresh endpoint AttributeError)
   - Fixed simulate endpoint validation errors (missing required fields)
   - Fixed status endpoint cache detection logic
   - Implemented recommendations caching system
   - Removed duplicate route definitions

2. Test Coverage:
   - All AI endpoints return proper responses without validation errors
   - Cache functionality works correctly across all endpoints
   - Status endpoint accurately reflects cache state
   - Recommendations are consistent when cached
   - Simulate endpoint returns complete valid objects
   - No duplicate route conflicts exist
"""

import asyncio
import requests
import json
import time
import traceback
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum


class TestResult(Enum):
    """Test result status"""
    PASS = "PASS"
    FAIL = "FAIL"
    WARNING = "WARNING"
    SKIPPED = "SKIPPED"


@dataclass
class TestCase:
    """Individual test case result"""
    name: str
    result: TestResult
    message: str
    details: Optional[str] = None
    response_time: Optional[float] = None
    status_code: Optional[int] = None


class AITestSuite:
    """Comprehensive AI API test suite"""

    def __init__(self, base_url: str = "http://localhost:8000/api"):
        self.base_url = base_url
        self.session = requests.Session()
        self.user_id = None
        self.test_results: List[TestCase] = []

        # Test data - use timestamp for unique email
        timestamp = int(time.time())
        self.test_user = {
            "email": f"ai_test_{timestamp}@debtease.com",
            "password": "AITest123!",
            "full_name": "AI Test User",
            "phone": "+91-8888888888",
            "monthly_income": 100000.0
        }

        self.test_debt = {
            "name": "AI Test Credit Card",
            "debt_type": "credit_card",
            "principal_amount": 75000.0,
            "current_balance": 50000.0,
            "interest_rate": 22.5,
            "minimum_payment": 2000.0,
            "due_date": "2024-04-15",
            "lender": "AI Test Bank",
            "payment_frequency": "monthly",
            "is_high_priority": True,
            "notes": "Test debt for AI validation"
        }

        # Expected response schemas for validation
        self.expected_schemas = {
            "insights": {
                "required_fields": ["debt_analysis", "recommendations", "metadata"],
                "debt_analysis_fields": ["total_debt", "debt_count", "average_interest_rate", "total_minimum_payments", "high_priority_count", "generated_at"],
                "recommendations_required": ["id", "user_id", "recommendation_type", "title", "description"],
                "metadata_fields": ["processing_time", "fallback_used", "errors", "generated_at"]
            },
            "status": {
                "required_fields": ["status", "cached", "message"],
                "status_values": ["completed", "expired", "stale", "expired_and_stale", "processing", "queued", "not_generated", "error"]
            },
            "simulate": {
                "required_fields": ["user_id", "simulation_results", "generated_at"],
                "result_fields": ["scenario_id", "time_to_debt_free", "total_interest_paid", "total_amount_paid", "debt_free_date", "valid"]
            }
        }

    def log_result(self, test_case: TestCase):
        """Log and store test result"""
        self.test_results.append(test_case)
        status_emoji = {
            TestResult.PASS: "âœ…",
            TestResult.FAIL: "âŒ",
            TestResult.WARNING: "âš ï¸",
            TestResult.SKIPPED: "â­ï¸"
        }

        emoji = status_emoji.get(test_case.result, "â“")
        print(f"{emoji} {test_case.name} - {test_case.result.value}")
        if test_case.message:
            print(f"   ğŸ“ {test_case.message}")
        if test_case.response_time:
            print(f"   â±ï¸  Response time: {test_case.response_time:.3f}s")
        if test_case.details:
            print(f"   ğŸ“‹ Details: {test_case.details}")
        print()

    def setup_test_user(self) -> bool:
        """Setup test user with authentication"""
        try:
            print("ğŸš€ Setting up test environment...")
            print("=" * 50)

            # Try to register user
            register_response = self.session.post(
                f"{self.base_url}/auth/register",
                json=self.test_user
            )

            if register_response.status_code not in [200, 201, 400]:  # 200 might also be returned, 400 might be user exists
                self.log_result(TestCase(
                    "User Registration",
                    TestResult.FAIL,
                    f"Registration failed with status {register_response.status_code}",
                    details=register_response.text[:200]
                ))
                return False

            # Login using form endpoint for Bearer token
            login_response = self.session.post(
                f"{self.base_url}/auth/login/form",
                data={
                    "username": self.test_user["email"],
                    "password": self.test_user["password"]
                }
            )

            if login_response.status_code != 200:
                self.log_result(TestCase(
                    "User Login",
                    TestResult.FAIL,
                    f"Login failed with status {login_response.status_code}",
                    details=login_response.text[:200]
                ))
                return False

            login_data = login_response.json()
            self.user_id = login_data.get("user", {}).get("id")

            if not self.user_id:
                self.log_result(TestCase(
                    "User ID Extraction",
                    TestResult.FAIL,
                    "Could not extract user ID from login response"
                ))
                return False

            # Extract session token from response for Bearer token auth
            session_token = login_data.get("access_token") or login_data.get("session_token")
            if session_token:
                # Set Authorization header for subsequent requests
                self.session.headers.update({
                    "Authorization": f"Bearer {session_token}"
                })
            else:
                # Try to get from cookie as fallback
                session_token = self.session.cookies.get('session_token')
                if not session_token:
                    self.log_result(TestCase(
                        "Session Token",
                        TestResult.FAIL,
                        "Login did not provide session token in response or cookie"
                    ))
                    return False
                else:
                    # Set Authorization header for subsequent requests
                    self.session.headers.update({
                        "Authorization": f"Bearer {session_token}"
                    })

            # Update user income for DTI calculations
            income_response = self.session.patch(
                f"{self.base_url}/users/profile",
                json={"monthly_income": self.test_user["monthly_income"]}
            )

            # Create test debt
            debt_response = self.session.post(
                f"{self.base_url}/debts/",
                json=self.test_debt
            )

            if debt_response.status_code != 201:
                self.log_result(TestCase(
                    "Test Debt Creation",
                    TestResult.FAIL,
                    f"Could not create test debt: {debt_response.status_code}",
                    details=debt_response.text[:200]
                ))
                return False

            self.log_result(TestCase(
                "Test Environment Setup",
                TestResult.PASS,
                f"Test user and debt created successfully (ID: {self.user_id})"
            ))

            return True

        except Exception as e:
            self.log_result(TestCase(
                "Test Environment Setup",
                TestResult.FAIL,
                f"Setup failed with exception: {str(e)}",
                details=traceback.format_exc()
            ))
            return False

    def test_endpoint(self,
                     endpoint: str,
                     method: str = "GET",
                     data: Optional[Dict] = None,
                     params: Optional[Dict] = None,
                     expected_status: int = 200) -> Tuple[bool, Optional[Dict], float, int]:
        """Test a single endpoint and return results"""
        try:
            start_time = time.time()

            if method.upper() == "GET":
                response = self.session.get(
                    f"{self.base_url}/ai{endpoint}",
                    params=params or {}
                )
            elif method.upper() == "POST":
                response = self.session.post(
                    f"{self.base_url}/ai{endpoint}",
                    json=data or {},
                    params=params or {}
                )
            else:
                raise ValueError(f"Unsupported method: {method}")

            response_time = time.time() - start_time

            # Check status code
            if response.status_code != expected_status:
                return False, None, response_time, response.status_code

            # Try to parse JSON response
            try:
                response_data = response.json()
                return True, response_data, response_time, response.status_code
            except json.JSONDecodeError:
                return False, {"error": "Invalid JSON response"}, response_time, response.status_code

        except Exception as e:
            return False, {"error": str(e)}, 0.0, 0

    def validate_response_schema(self, data: Dict, schema_key: str) -> Tuple[bool, str]:
        """Validate response matches expected schema"""
        try:
            schema = self.expected_schemas.get(schema_key, {})

            # Check required fields
            required_fields = schema.get("required_fields", [])
            for field in required_fields:
                if field not in data:
                    return False, f"Missing required field: {field}"

            # Specific validations for different schemas
            if schema_key == "insights":
                # Validate debt_analysis structure
                debt_analysis = data.get("debt_analysis", {})
                for field in schema.get("debt_analysis_fields", []):
                    if field not in debt_analysis:
                        return False, f"Missing debt_analysis field: {field}"

                # Validate recommendations structure
                recommendations = data.get("recommendations", [])
                if recommendations:  # Only validate if recommendations exist
                    for i, rec in enumerate(recommendations):
                        for field in schema.get("recommendations_required", []):
                            if field not in rec:
                                return False, f"Missing recommendation[{i}] field: {field}"

                # Validate metadata structure
                metadata = data.get("metadata", {})
                for field in schema.get("metadata_fields", []):
                    if field not in metadata:
                        return False, f"Missing metadata field: {field}"

            elif schema_key == "status":
                # Validate status value is acceptable
                status = data.get("status", "")
                valid_statuses = schema.get("status_values", [])
                if status not in valid_statuses:
                    return False, f"Invalid status value: {status}. Expected one of {valid_statuses}"

            elif schema_key == "simulate":
                # Validate simulation results structure
                results = data.get("simulation_results", [])
                if results:
                    for i, result in enumerate(results):
                        for field in schema.get("result_fields", []):
                            if field not in result:
                                return False, f"Missing simulation_results[{i}] field: {field}"

            return True, "Schema validation passed"

        except Exception as e:
            return False, f"Schema validation error: {str(e)}"

    def test_basic_insights(self):
        """Test /api/ai/insights endpoint"""
        print("ğŸ” Testing Basic AI Insights Endpoint")
        print("-" * 40)

        success, data, response_time, status_code = self.test_endpoint("/insights")

        if not success:
            self.log_result(TestCase(
                "Basic AI Insights - Request",
                TestResult.FAIL,
                f"Request failed with status {status_code}",
                details=str(data) if data else None,
                response_time=response_time,
                status_code=status_code
            ))
            return

        # Validate response schema
        schema_valid, schema_msg = self.validate_response_schema(data, "insights")
        if not schema_valid:
            self.log_result(TestCase(
                "Basic AI Insights - Schema",
                TestResult.FAIL,
                f"Schema validation failed: {schema_msg}",
                response_time=response_time,
                status_code=status_code
            ))
            return

        self.log_result(TestCase(
            "Basic AI Insights",
            TestResult.PASS,
            f"Endpoint working correctly",
            details=f"Debt count: {data.get('debt_analysis', {}).get('debt_count', 0)}, Recommendations: {len(data.get('recommendations', []))}",
            response_time=response_time,
            status_code=status_code
        ))

    def test_enhanced_insights(self):
        """Test /api/ai/insights/enhanced endpoint"""
        print("ğŸ” Testing Enhanced AI Insights Endpoint")
        print("-" * 40)

        success, data, response_time, status_code = self.test_endpoint("/insights/enhanced")

        if not success:
            self.log_result(TestCase(
                "Enhanced AI Insights - Request",
                TestResult.FAIL,
                f"Request failed with status {status_code}",
                details=str(data) if data else None,
                response_time=response_time,
                status_code=status_code
            ))
            return

        # Check for enhanced-specific fields
        required_enhanced_fields = ["insights", "recommendations", "dtiAnalysis"]
        missing_fields = [field for field in required_enhanced_fields if field not in data]

        if missing_fields:
            self.log_result(TestCase(
                "Enhanced AI Insights - Structure",
                TestResult.WARNING,
                f"Missing enhanced fields: {missing_fields}",
                response_time=response_time,
                status_code=status_code
            ))
        else:
            self.log_result(TestCase(
                "Enhanced AI Insights",
                TestResult.PASS,
                f"Enhanced endpoint working correctly",
                response_time=response_time,
                status_code=status_code
            ))

    def test_status_endpoint(self):
        """Test /api/ai/insights/status endpoint"""
        print("ğŸ” Testing Status Endpoint")
        print("-" * 40)

        success, data, response_time, status_code = self.test_endpoint("/insights/status")

        if not success:
            self.log_result(TestCase(
                "Status Endpoint - Request",
                TestResult.FAIL,
                f"Request failed with status {status_code}",
                details=str(data) if data else None,
                response_time=response_time,
                status_code=status_code
            ))
            return

        # Validate schema
        schema_valid, schema_msg = self.validate_response_schema(data, "status")
        if not schema_valid:
            self.log_result(TestCase(
                "Status Endpoint - Schema",
                TestResult.FAIL,
                f"Schema validation failed: {schema_msg}",
                response_time=response_time,
                status_code=status_code
            ))
            return

        # Check if status is not always "not_generated" (this was the bug)
        status_value = data.get("status", "")
        if status_value == "not_generated":
            self.log_result(TestCase(
                "Status Endpoint - Logic",
                TestResult.WARNING,
                f"Status is 'not_generated' - cache detection may not be working properly",
                details=f"Full response: {json.dumps(data, indent=2)}",
                response_time=response_time,
                status_code=status_code
            ))
        else:
            self.log_result(TestCase(
                "Status Endpoint - Logic",
                TestResult.PASS,
                f"Status correctly detected as '{status_value}'",
                details=f"Cached: {data.get('cached', False)}",
                response_time=response_time,
                status_code=status_code
            ))

        self.log_result(TestCase(
            "Status Endpoint",
            TestResult.PASS,
            f"Status endpoint working correctly",
            response_time=response_time,
            status_code=status_code
        ))

    def test_refresh_endpoint(self):
        """Test /api/ai/insights/refresh endpoint - should not crash with AttributeError"""
        print("ğŸ” Testing Refresh Endpoint (AttributeError Fix)")
        print("-" * 40)

        success, data, response_time, status_code = self.test_endpoint("/insights/refresh", method="POST")

        if not success:
            error_msg = str(data) if data else "Unknown error"
            # Check if it's the specific AttributeError we're testing for
            if "AttributeError" in error_msg and "to_dict" in error_msg:
                self.log_result(TestCase(
                    "Refresh Endpoint - AttributeError Fix",
                    TestResult.FAIL,
                    f"Still getting AttributeError related to to_dict method",
                    details=error_msg,
                    response_time=response_time,
                    status_code=status_code
                ))
            else:
                self.log_result(TestCase(
                    "Refresh Endpoint - Request",
                    TestResult.FAIL,
                    f"Request failed with status {status_code}",
                    details=error_msg,
                    response_time=response_time,
                    status_code=status_code
                ))
            return

        self.log_result(TestCase(
            "Refresh Endpoint - AttributeError Fix",
            TestResult.PASS,
            f"No AttributeError - to_dict() method working correctly",
            response_time=response_time,
            status_code=status_code
        ))

    def test_recommendations_endpoint(self):
        """Test /api/ai/recommendations endpoint"""
        print("ğŸ” Testing Recommendations Endpoint")
        print("-" * 40)

        success, data, response_time, status_code = self.test_endpoint("/recommendations")

        if not success:
            self.log_result(TestCase(
                "Recommendations Endpoint - Request",
                TestResult.FAIL,
                f"Request failed with status {status_code}",
                details=str(data) if data else None,
                response_time=response_time,
                status_code=status_code
            ))
            return

        # Validate that we get a list
        if not isinstance(data, list):
            self.log_result(TestCase(
                "Recommendations Endpoint - Response Type",
                TestResult.FAIL,
                f"Expected list, got {type(data).__name__}",
                response_time=response_time,
                status_code=status_code
            ))
            return

        self.log_result(TestCase(
            "Recommendations Endpoint",
            TestResult.PASS,
            f"Returned {len(data)} recommendations",
            response_time=response_time,
            status_code=status_code
        ))

    def test_recommendations_consistency(self):
        """Test that recommendations are consistent across multiple calls (caching)"""
        print("ğŸ” Testing Recommendations Consistency (Caching)")
        print("-" * 40)

        # Make first call
        success1, data1, _, _ = self.test_endpoint("/recommendations")
        if not success1:
            self.log_result(TestCase(
                "Recommendations Consistency - First Call",
                TestResult.FAIL,
                "First call failed"
            ))
            return

        # Wait a moment
        time.sleep(1)

        # Make second call
        success2, data2, _, _ = self.test_endpoint("/recommendations")
        if not success2:
            self.log_result(TestCase(
                "Recommendations Consistency - Second Call",
                TestResult.FAIL,
                "Second call failed"
            ))
            return

        # Compare results
        if len(data1) == len(data2):
            # Check if recommendations are the same (by ID or content)
            if data1 == data2:
                self.log_result(TestCase(
                    "Recommendations Consistency",
                    TestResult.PASS,
                    f"Recommendations are consistent across calls (caching working)",
                    details=f"Both calls returned {len(data1)} identical recommendations"
                ))
            else:
                self.log_result(TestCase(
                    "Recommendations Consistency",
                    TestResult.WARNING,
                    f"Recommendations differ between calls",
                    details=f"Call 1: {len(data1)} recs, Call 2: {len(data2)} recs, but content differs"
                ))
        else:
            self.log_result(TestCase(
                "Recommendations Consistency",
                TestResult.WARNING,
                f"Different number of recommendations returned",
                details=f"Call 1: {len(data1)} recs, Call 2: {len(data2)} recs"
            ))

    def test_simulate_endpoint(self):
        """Test /api/ai/simulate endpoint - should return complete valid objects"""
        print("ğŸ” Testing Simulate Endpoint (Validation Fix)")
        print("-" * 40)

        # Test data for simulation
        simulation_data = {
            "scenarios": [
                {
                    "monthly_payment": 3000.0,
                    "strategy": "avalanche"
                },
                {
                    "monthly_payment": 4000.0,
                    "strategy": "snowball"
                }
            ]
        }

        success, data, response_time, status_code = self.test_endpoint(
            "/simulate",
            method="POST",
            data=simulation_data
        )

        if not success:
            self.log_result(TestCase(
                "Simulate Endpoint - Request",
                TestResult.FAIL,
                f"Request failed with status {status_code}",
                details=str(data) if data else None,
                response_time=response_time,
                status_code=status_code
            ))
            return

        # Validate response schema
        schema_valid, schema_msg = self.validate_response_schema(data, "simulate")
        if not schema_valid:
            self.log_result(TestCase(
                "Simulate Endpoint - Schema",
                TestResult.FAIL,
                f"Schema validation failed: {schema_msg}",
                response_time=response_time,
                status_code=status_code
            ))
            return

        # Check that we got results for both scenarios
        results = data.get("simulation_results", [])
        if len(results) != len(simulation_data["scenarios"]):
            self.log_result(TestCase(
                "Simulate Endpoint - Result Count",
                TestResult.WARNING,
                f"Expected {len(simulation_data['scenarios'])} results, got {len(results)}",
                response_time=response_time,
                status_code=status_code
            ))

        self.log_result(TestCase(
            "Simulate Endpoint - Validation Fix",
            TestResult.PASS,
            f"Simulation completed successfully with {len(results)} results",
            details=f"All required fields present in simulation objects",
            response_time=response_time,
            status_code=status_code
        ))

    def test_other_endpoints(self):
        """Test remaining AI endpoints"""
        print("ğŸ” Testing Other AI Endpoints")
        print("-" * 40)

        # Test strategy comparison
        success, _, response_time, status_code = self.test_endpoint(
            "/strategies/compare",
            params={"monthly_payment": 3500.0}
        )

        self.log_result(TestCase(
            "Strategy Comparison Endpoint",
            TestResult.PASS if success else TestResult.FAIL,
            f"Status: {status_code}" if success else f"Failed with {status_code}",
            response_time=response_time,
            status_code=status_code
        ))

        # Test payment timeline
        success, _, response_time, status_code = self.test_endpoint(
            "/timeline",
            params={"monthly_payment": 3500.0, "strategy": "avalanche"}
        )

        self.log_result(TestCase(
            "Payment Timeline Endpoint",
            TestResult.PASS if success else TestResult.FAIL,
            f"Status: {status_code}" if success else f"Failed with {status_code}",
            response_time=response_time,
            status_code=status_code
        ))

        # Test optimization metrics
        optimization_data = {
            "current_strategy": {"monthly_payment": 2500.0, "strategy": "avalanche"},
            "optimized_strategy": {"monthly_payment": 3500.0, "strategy": "avalanche"}
        }

        success, _, response_time, status_code = self.test_endpoint(
            "/optimize",
            method="POST",
            data=optimization_data
        )

        self.log_result(TestCase(
            "Optimization Metrics Endpoint",
            TestResult.PASS if success else TestResult.FAIL,
            f"Status: {status_code}" if success else f"Failed with {status_code}",
            response_time=response_time,
            status_code=status_code
        ))

        # Test DTI analysis
        success, _, response_time, status_code = self.test_endpoint("/dti")

        self.log_result(TestCase(
            "DTI Analysis Endpoint",
            TestResult.PASS if success else TestResult.FAIL,
            f"Status: {status_code}" if success else f"Failed with {status_code}",
            response_time=response_time,
            status_code=status_code
        ))

    def test_no_duplicate_routes(self):
        """Test that there are no duplicate route conflicts"""
        print("ğŸ” Testing for Duplicate Route Conflicts")
        print("-" * 40)

        # This test checks if duplicate routes cause conflicts by testing the same endpoint multiple times
        test_urls = [
            "/insights",
            "/recommendations",
            "/simulate",
            "/insights/status"
        ]

        all_passed = True
        for url in test_urls:
            # Test each endpoint twice quickly to check for route conflicts
            success1, _, _, status1 = self.test_endpoint(url, method="POST" if url == "/simulate" else "GET",
                                                       data={"scenarios": [{"monthly_payment": 3000.0, "strategy": "avalanche"}]} if url == "/simulate" else None)
            success2, _, _, status2 = self.test_endpoint(url, method="POST" if url == "/simulate" else "GET",
                                                       data={"scenarios": [{"monthly_payment": 3000.0, "strategy": "avalanche"}]} if url == "/simulate" else None)

            if status1 != status2:
                all_passed = False
                self.log_result(TestCase(
                    f"Duplicate Route Check - {url}",
                    TestResult.FAIL,
                    f"Inconsistent responses: {status1} vs {status2}",
                    details="Possible duplicate route conflict"
                ))

        if all_passed:
            self.log_result(TestCase(
                "Duplicate Route Conflicts",
                TestResult.PASS,
                "No duplicate route conflicts detected",
                details=f"Tested {len(test_urls)} endpoints consistently"
            ))

    def run_comprehensive_tests(self):
        """Run all tests in the comprehensive suite"""
        print("ğŸ§ª COMPREHENSIVE AI INSIGHTS API TEST SUITE")
        print("=" * 60)
        print(f"ğŸ¯ Target: {self.base_url}")
        print(f"ğŸ• Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)

        # Setup test environment
        if not self.setup_test_user():
            print("âŒ Test environment setup failed. Cannot continue.")
            return False

        print("\nğŸ”¬ RUNNING COMPREHENSIVE TEST SUITE")
        print("=" * 60)

        # Run all test cases
        self.test_basic_insights()
        self.test_enhanced_insights()
        self.test_status_endpoint()
        self.test_refresh_endpoint()  # Critical test for AttributeError fix
        self.test_recommendations_endpoint()
        self.test_recommendations_consistency()  # Test caching
        self.test_simulate_endpoint()  # Critical test for validation fix
        self.test_other_endpoints()
        self.test_no_duplicate_routes()

        return True

    def generate_report(self):
        """Generate comprehensive test report"""
        print("\nğŸ“Š COMPREHENSIVE TEST RESULTS REPORT")
        print("=" * 60)

        # Count results by type
        results_count = {
            TestResult.PASS: sum(1 for r in self.test_results if r.result == TestResult.PASS),
            TestResult.FAIL: sum(1 for r in self.test_results if r.result == TestResult.FAIL),
            TestResult.WARNING: sum(1 for r in self.test_results if r.result == TestResult.WARNING),
            TestResult.SKIPPED: sum(1 for r in self.test_results if r.result == TestResult.SKIPPED)
        }

        total_tests = len(self.test_results)
        success_rate = (results_count[TestResult.PASS] / total_tests * 100) if total_tests > 0 else 0

        print(f"ğŸ“ˆ OVERALL SUMMARY:")
        print(f"   âœ… Passed: {results_count[TestResult.PASS]}")
        print(f"   âŒ Failed: {results_count[TestResult.FAIL]}")
        print(f"   âš ï¸  Warnings: {results_count[TestResult.WARNING]}")
        print(f"   â­ï¸  Skipped: {results_count[TestResult.SKIPPED]}")
        print(f"   ğŸ“Š Success Rate: {success_rate:.1f}%")
        print()

        # Critical fixes validation
        print("ğŸ”§ CRITICAL FIXES VALIDATION:")
        print("-" * 30)

        critical_fixes = {
            "to_dict() AttributeError Fix": any("AttributeError Fix" in r.name and r.result == TestResult.PASS for r in self.test_results),
            "Simulate Validation Fix": any("Simulate Endpoint - Validation Fix" in r.name and r.result == TestResult.PASS for r in self.test_results),
            "Status Endpoint Logic": any("Status Endpoint - Logic" in r.name and r.result in [TestResult.PASS, TestResult.WARNING] for r in self.test_results),
            "Recommendations Caching": any("Recommendations Consistency" in r.name and r.result in [TestResult.PASS, TestResult.WARNING] for r in self.test_results),
            "No Duplicate Routes": any("Duplicate Route Conflicts" in r.name and r.result == TestResult.PASS for r in self.test_results)
        }

        for fix, validated in critical_fixes.items():
            status = "âœ… VALIDATED" if validated else "âŒ ISSUE REMAINS"
            print(f"   {status}: {fix}")

        print()

        # Failed tests details
        failed_tests = [r for r in self.test_results if r.result == TestResult.FAIL]
        if failed_tests:
            print("âŒ FAILED TESTS DETAILS:")
            print("-" * 30)
            for test in failed_tests:
                print(f"   ğŸ”´ {test.name}")
                print(f"      ğŸ’¬ {test.message}")
                if test.details:
                    print(f"      ğŸ“‹ {test.details[:100]}...")
                print()

        # Warning tests details
        warning_tests = [r for r in self.test_results if r.result == TestResult.WARNING]
        if warning_tests:
            print("âš ï¸  WARNING TESTS DETAILS:")
            print("-" * 30)
            for test in warning_tests:
                print(f"   ğŸŸ¡ {test.name}")
                print(f"      ğŸ’¬ {test.message}")
                if test.details:
                    print(f"      ğŸ“‹ {test.details[:100]}...")
                print()

        # Performance summary
        timed_tests = [r for r in self.test_results if r.response_time is not None]
        if timed_tests:
            avg_response_time = sum(r.response_time for r in timed_tests) / len(timed_tests)
            max_response_time = max(r.response_time for r in timed_tests)
            print("â±ï¸  PERFORMANCE SUMMARY:")
            print(f"   ğŸ“Š Average response time: {avg_response_time:.3f}s")
            print(f"   ğŸ“Š Maximum response time: {max_response_time:.3f}s")
            print()

        # Recommendations
        print("ğŸ’¡ RECOMMENDATIONS:")
        print("-" * 20)

        if results_count[TestResult.FAIL] == 0:
            print("   ğŸ‰ All critical tests passed! API fixes are working correctly.")
        else:
            print("   ğŸ”§ Some issues remain that need attention:")
            for test in failed_tests[:3]:  # Show first 3 failures
                print(f"      - {test.name}: {test.message}")

        if results_count[TestResult.WARNING] > 0:
            print("   âš ï¸  Some warnings detected - review for potential improvements:")
            for test in warning_tests[:3]:  # Show first 3 warnings
                print(f"      - {test.name}: {test.message}")

        print()
        print("=" * 60)
        print(f"ğŸ Test completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)

        return results_count[TestResult.FAIL] == 0  # Return True if no failures


def main():
    """Main function to run the comprehensive test suite"""
    print("ğŸš€ DebtEase AI Insights API Comprehensive Test Suite")
    print("ğŸ¯ Validating all critical fixes and functionality")
    print()

    # Initialize test suite
    test_suite = AITestSuite()

    try:
        # Run comprehensive tests
        success = test_suite.run_comprehensive_tests()

        if success:
            # Generate report
            all_passed = test_suite.generate_report()

            if all_passed:
                print("\nğŸ‰ ALL TESTS PASSED! ğŸ‰")
                print("ğŸ”§ All critical AI insights API fixes have been validated successfully!")
                return 0
            else:
                print("\nâš ï¸  SOME TESTS FAILED")
                print("ğŸ”§ Some issues still need to be addressed")
                return 1
        else:
            print("\nâŒ TEST ENVIRONMENT SETUP FAILED")
            print("ğŸ”§ Cannot run comprehensive tests without proper setup")
            return 2

    except KeyboardInterrupt:
        print("\nâ¹ï¸  Test suite interrupted by user")
        return 130
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {str(e)}")
        traceback.print_exc()
        return 3


if __name__ == "__main__":
    exit(main())